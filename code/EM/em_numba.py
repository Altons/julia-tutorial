"""EM
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1yyrKFzwoZQfS2J7DiIiEONsMHjJhDsCb
"""

import numpy as np  # maths in general
from numpy.linalg import slogdet, det, solve  # matrix operations
import matplotlib.pyplot as plt  # plotting
from scipy.stats import multivariate_normal as dmvnorm  # multiv. normal
from numba import njit
import pandas as pd

# samples = np.loadtxt('samples.txt')
# X = samples['data']          # 2 columns of data to be clustered
# pi0 = samples['pi0']         # The priors for the mixture proportions
# mu0 = samples['mu0']         # The start values for \mu_c
# sigma0 = samples['sigma0']   # Covariance matrix between the clusters

df = pd.read_csv('code/EM/samples.txt', index_col=0, sep=" ")
# print(df)
X = df.values
pi = np.array([0.34518140, 0.6066179, 0.04820071])  # The priors for the mixture proportions
mu = np.array(
    [[-0.7133619, 0.90635089], [0.7662367, 0.8260541], [-1.3236828, -1.7524445]])  # The start values for \mu_c
sigma = np.array([[[1.004904, 1.899802], [1.899802, 4.183546]], [[1.9686781, 0.7841534], [0.7841534, 1.8331994]],
    [[0.1931634, -0.1164864], [-0.1164864, 1.9839597]], ])  # Covariance matrix between the clusters

plt.clf()  # clean plot environment x
plt.figure(1)
plt.figure(figsize=(14, 10))
plt.scatter(X[:, 0], X[:, 1], c='orange', s=30)
plt.axis('equal')
plt.show()

@njit
def mvnormal_pdf(mu, sigma, dim, x):
    return 1/(np.sqrt((2*np.pi)**dim*np.linalg.det(sigma)))*np.exp(-.5*(x - mu)@np.linalg.inv(sigma)@(x - mu))

@njit
def logpdf(x, mu, sigma):
    return np.log(mvnormal_pdf(mu, sigma, len(mu) , x))

@njit
def E_step(X, pi, mu, sigma):
    """
    Performs E-step on GMM model
    Each input is numpy array:
    X: (N x d), data points
    pi: (C), mixture component weights
    mu: (C x d), mixture component means
    sigma: (C x d x d), mixture component covariance matrices

    Returns:
    gamma: (N x C), probabilities of clusters for objects
    """
    N = X.shape[0]
    C = pi.shape[0]
    d = mu.shape[1]
    gamma = np.asfortranarray(np.zeros((N, C)))
    #     gamma = np.zeros((N, C))
    #     gamma_div = np.zeros((N, 1))
    for c in range(C):
        # Normalization constant of a biv. Normal distribution
        const = ((2 * 3.141592) ** d * det(sigma[c])) ** 0.5
        # Subtracts mu's of the observations
        x = np.subtract(X, mu[c]).transpose()
        # Calculating the core of the biv. Normal distribution
        # with the inverse of Sigma * X being found with
        # since (SS^-1 = I):
        invSx = np.linalg.solve(sigma[c], x)
        xInvSx = -1 / 2 * np.multiply(x, invSx).sum(0)
        density = np.exp(xInvSx) / const
        # saving final values for all x with all the mu's
        gamma[:, c] = pi[c] * density
    #         gamma_div[:,0] += gamma[:, c]
    gamma /= gamma.sum(1).reshape(N, 1)  # TO IMPROVE
    #     gamma /= gamma_div
    return gamma


@njit
def M_step(X, gamma):
    """
    Performs M-step on GMM model
    Each input is numpy array:
    X: (N x d), data pointsobjmode
    gamma: (N x C), distribution q(T)

    Returns:
    pi: (C)
    mu: (C x d)
    sigma: (C x d x d)
    """
    N = X.shape[0]  # number of objects
    C = gamma.shape[1]  # number of clusters
    d = X.shape[1]  # dimension of each object
    pi = np.zeros((C))
    mu = np.zeros((C, d))
    sigma = np.zeros((C, d, d))
    gamma_sum = gamma.sum(0)
    # the new pi's are:
    pi = gamma_sum / N
    for c in range(C):
        mu[c] = np.dot(gamma[:, c].transpose(), X) / gamma_sum[c]
        # # sum of probabilities * x/sum(probabilities)
        x = np.subtract(X, mu[c])
        sigma[c] = (x.transpose() @ np.diag(gamma[:, c]) @ x) / gamma_sum[
            c]  #   # mu_multp = mu[c] * mu[c].transpose()  # x = np.matrix(X)  # xpxt = (x.transpose() * np.diag(gamma[:, c]) * x)  # norm_xpxt = xxt/gamma_sum[c]  # sigma[c] = norm_xpxt - mu_multp/gamma_sum[c]
    return pi, mu, sigma


gamma_E = E_step(X, pi, mu, sigma)
pi, mu, sigma = M_step(X, gamma_E)


@njit
def compute_vlb(X, pi, mu, sigma, gamma):
    """
    Each input is numpy array:
    X: (N x d), data points
    gamma: (N x C), distribution q(T)
    pi: (C)
    mu: (C x d)
    sigma: (C x d x d)

    Returns value of variational lower bound
    """
    N = X.shape[0]  # number of objects
    C = gamma.shape[1]  # number of clusters
    d = X.shape[1]  # dimension of each object
    loss = 0
    for c in range(C):
        for n in range(N):
            # calculates log density of x | theta
            #             with objmode(log_dmvnorm_c='float64'):
            #                 log_dmvnorm_c = dmvnorm.logpdf(X[n,:], mu[c], sigma[c])
            log_dmvnorm_c = logpdf(X[n, :], mu[c], sigma[c])
            # increasing loss with the formula
            loss += gamma[n, c] * ((np.log(pi[c]) + log_dmvnorm_c) - np.log(gamma[n, c]))
    return loss


loss = compute_vlb(X, pi, mu, sigma, gamma_E)


@njit
def train_EM(X, C, rtol=1e-3, max_iter=100, restarts=10):
    '''
    Starts with random initialization *restarts* times
    Runs optimization until saturation with *rtol* reached
    or *max_iter* iterations were made.

    X: (N, d), data points
    C: int, number of clusters
    '''
    N = X.shape[0]  # number of objects
    d = X.shape[1]  # dimension of each object
    best_loss = -np.inf
    best_pi = None
    best_mu = None
    best_sigma = None
    for _ in range(restarts):
        #         try:
        pi = np.random.rand(C)
        pi = pi / pi.sum(0)
        mu = np.random.random_sample((C, d))
        sigma = np.empty((C, d, d))
        for i in range(C):
            sigma[i] = np.eye(d)
        loss = -np.inf
        should_continue = True
        iteration = 0
        for _ in range(max_iter):
            former_loss = loss
            gamma = E_step(X, pi, mu, sigma)
            pi, mu, sigma = M_step(X, gamma)
            loss = compute_vlb(X, pi, mu, sigma, gamma)
            # checking that the loss actually improved (not a bug)
            if (former_loss > loss):
                print("bug")
            # is the increase in the loss actually different from
            # the previous one?
            should_continue = np.abs(loss / former_loss - 1) > rtol
            if (should_continue == False):
                break  # is the current loss better than -infinite? if yes, save the
        # results
        if (loss > best_loss):
            best_loss = loss
            best_pi = pi
            best_mu = mu
            best_sigma = sigma
            print(best_loss)
    #         except np.linalg.LinAlgError:
    #             print("Singular matrix: components collapsed")
    #             pass
    return best_loss, best_pi, best_mu, best_sigma




best_loss, best_pi, best_mu, best_sigma = train_EM(X, 3, restarts=20)

plt.clf()
plt.figure(1)
plt.figure(figsize=(8, 7))
# Now, what is the E-step for the current optimal parameters?
gamma = E_step(X, best_pi, best_mu, best_sigma)
labels = gamma.argmax(1)
plt.scatter(X[:, 0], X[:, 1], c=labels, s=30)
plt.axis('equal')
plt.show()

import time

total_time = []

for i in range(1, 10):
    start_time = time.time()
    best_loss, best_pi, best_mu, best_sigma = train_EM(X, 3, restarts=20)
    total_time.append(time.time() - start_time)

# print("--- %s seconds ---" % (time.time() - start_time))

print([min(total_time), np.median(total_time), np.mean(total_time),
 max(total_time)])  # [41.121864795684814, 46.80030274391174, 46.705216566721596, 53.73479151725769] s

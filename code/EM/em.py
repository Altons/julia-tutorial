# -*- coding: utf-8 -*-
"""EM

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yyrKFzwoZQfS2J7DiIiEONsMHjJhDsCb
"""

import numpy as np                                    # maths in general 
from numpy.linalg import slogdet, det, solve           # matrix operations
import matplotlib.pyplot as plt                        # plotting
from scipy.stats import multivariate_normal as dmvnorm # multiv. normal

samples = np.load('code/EM/samples.npz')
X = samples['data']          # 2 columns of data to be clustered
pi0 = samples['pi0']         # The priors for the mixture proportions
mu0 = samples['mu0']         # The start values for \mu_c 
sigma0 = samples['sigma0']   # Covariance matrix between the clusters
plt.clf() # clean plot environment x
plt.figure(1)
plt.figure(figsize = (14, 10))
plt.scatter(X[:, 0], X[:, 1], c = 'orange', s = 30)
plt.axis('equal')
plt.show()

def E_step(X, pi, mu, sigma):
    """
    Performs E-step on GMM model
    Each input is numpy array:
    X: (N x d), data points
    pi: (C), mixture component weights 
    mu: (C x d), mixture component means
    sigma: (C x d x d), mixture component covariance matrices
    
    Returns:
    gamma: (N x C), probabilities of clusters for objects
    """
    N = X.shape[0] 
    C = pi.shape[0]
    d = mu.shape[1]
    gamma = np.zeros((N, C))
    for c in np.array(range(C)):        
        # Normalization constant of a biv. Normal distribution
        const = ((2 * 3.141592)**d * det(sigma[c]))**0.5
        # Subtracts mu's of the observations 
        x = np.subtract(X, mu[c]).transpose()
        # Calculating the core of the biv. Normal distribution
        # with the inverse of Sigma * X being found with
        # since (SS^-1 = I): 
        invSx = np.linalg.solve(sigma[c], x)
        xInvSx = -1/2 * np.multiply(x, invSx).sum(0)
        density = np.exp(xInvSx)/const
        # saving final values for all x with all the mu's
        gamma[:, c] = pi[c] * density
    gamma /= gamma.sum(1).reshape(N, 1)
    return gamma

def M_step(X, gamma):
    """
    Performs M-step on GMM model
    Each input is numpy array:
    X: (N x d), data points
    gamma: (N x C), distribution q(T)  
    
    Returns:
    pi: (C)
    mu: (C x d)
    sigma: (C x d x d)
    """
    N = X.shape[0] # number of objects
    C = gamma.shape[1] # number of clusters
    d = X.shape[1] # dimension of each object
    pi = np.zeros((C))
    mu = np.zeros((C, d)) 
    sigma = np.zeros((C, d, d)) 
    gamma_sum = gamma.sum(0)
    # the new pi's are:
    pi = gamma_sum/N
    for c in np.array(range(C)):
        mu[c] = np.dot(gamma[:, c].transpose(), X)/gamma_sum[c]
        # # sum of probabilities * x/sum(probabilities)
        x = np.matrix(np.subtract(X, mu[c]))
        sigma[c] = (x.transpose() * np.diag(gamma[:, c]) * x)/gamma_sum[c]
        # 
        # mu_multp = mu[c] * mu[c].transpose()
        # x = np.matrix(X)
        # xpxt = (x.transpose() * np.diag(gamma[:, c]) * x)
        # norm_xpxt = xxt/gamma_sum[c]
        # sigma[c] = norm_xpxt - mu_multp/gamma_sum[c]
    return pi, mu, sigma
gamma_E = E_step(X, pi0, mu0, sigma0)    
pi, mu, sigma = M_step(X, gamma_E)

def compute_vlb(X, pi, mu, sigma, gamma):
    """
    Each input is numpy array:
    X: (N x d), data points
    gamma: (N x C), distribution q(T)  
    pi: (C)
    mu: (C x d)
    sigma: (C x d x d)
    
    Returns value of variational lower bound
    """
    N = X.shape[0] # number of objects
    C = gamma.shape[1] # number of clusters
    d = X.shape[1] # dimension of each object
    loss = 0
    for c in np.array(range(C)):
        for n in np.array(range(N)):
            # calculates log density of x | theta
            log_dmvnorm_c = dmvnorm.logpdf(X[n,:], mu[c], sigma[c])
            # increasing loss with the formula
            loss += gamma[n, c] * ((np.log(pi[c]) + log_dmvnorm_c) - np.log(gamma[n, c]))
    return loss
loss = compute_vlb(X, pi, mu, sigma, gamma_E)

def train_EM(X, C, rtol = 1e-3, max_iter = 100, restarts = 10):
    '''
    Starts with random initialization *restarts* times
    Runs optimization until saturation with *rtol* reached
    or *max_iter* iterations were made.
    
    X: (N, d), data points
    C: int, number of clusters
    '''
    N = X.shape[0] # number of objects
    d = X.shape[1] # dimension of each object
    best_loss = -np.inf
    best_pi = None
    best_mu = None
    best_sigma = None
    for _ in range(restarts):
        try:
            pi = np.random.rand(C)
            pi = pi/pi.sum(0)
            mu = np.random.random_sample((C,d))
            sigma = np.array( [np.eye(d) for _ in range(C)])
            loss = -np.inf
            should_continue = True
            iteration = 0
            for _ in range(max_iter):
                former_loss = np.copy(loss)
                gamma = E_step(X, pi, mu, sigma)
                pi, mu, sigma = M_step(X, gamma)
                loss = compute_vlb(X, pi, mu, sigma, gamma)
                # checking that the loss actually improved (not a bug)
                if(former_loss > loss):
                    print("bug")
                # is the increase in the loss actually different from
                # the previous one?
                should_continue = np.abs(loss/former_loss - 1) > rtol
                if(should_continue == False):
                    break 
            # is the current loss better than -infinite? if yes, save the 
            # results
            if(loss > best_loss):
                best_loss = np.copy(loss)
                best_pi = pi
                best_mu = mu
                best_sigma = sigma
                print(best_loss)
        except np.linalg.LinAlgError:
            print("Singular matrix: components collapsed")
            pass
    return best_loss, best_pi, best_mu, best_sigma
best_loss, best_pi, best_mu, best_sigma = train_EM(X, 3, restarts = 20)

plt.clf()
plt.figure(1)
plt.figure(figsize = (8, 7))
# Now, what is the E-step for the current optimal parameters?
gamma = E_step(X, best_pi, best_mu, best_sigma)
labels = gamma.argmax(1)
plt.scatter(X[:, 0], X[:, 1], c = labels, s = 30)
plt.axis('equal')
plt.show()

import time
total_time = []

for i in range(1, 10):
  start_time = time.time()
  best_loss, best_pi, best_mu, best_sigma = train_EM(X, 3, restarts = 20)
  total_time.append(time.time() - start_time)

#print("--- %s seconds ---" % (time.time() - start_time))

[min(total_time), np.median(total_time), np.mean(total_time), max(total_time)]
# [41.121864795684814, 46.80030274391174, 46.705216566721596, 53.73479151725769] s
